\chapter{I Dataset}
In questa sezione vengono introdotti i dataset usati per i nostri
esperimenti. Il primo dei due è il FEMNIST, cioè l'EMNIST 
in cui ogni lettera o numero sono raggruppati in base alla 
persona che li ha scritti; il secondo è l'UCI HAR un dataset per il
riconoscimento dell'attività dell'utente in base a misurazioni fatte
da un dispositivo che ha addosso, come l'accellerometro di smartphones
o smart watches.

\section{FEMNIST}
Il famoso dataset MNIST (Modified National Institute of Standards and
Technology), l'Hello World del Machine Learning, nasce nel 1994 creato 
da LeCun et al.~\cite{lecun1998mnist} partendo dai un altro dataset 
del NIST.
Il MNIST è stato creato a partire dai dataset SD-1 e SD-3 (Special
Dataset). Lo SD-3 veniva usato come training set e conteneva immagini
di cifre scritte a mano da impiegati dell'American Census Bureau,
mentre lo SD-1, usato come test set, erano cifre scritte a mano da 
studenti delle scuole superiori americane. Le cifre degli studenti
erano però scritte con una calligrafia più difficile da leggere, rendendo più 
difficile trarre conclusioni sulle capacità del modello alla fine del
training. Per questo motivo è stato creato un unico dataset, l'MNIST 
che contiene un totale di 60'000 immagini di cifre sia degli studenti
che degli impiegati dell'American Census Bureau.

L'EMNIST (Extended MNIST) è un'estensione del dataset originale,
pubblicata nel 2017 da Cohen et al.~\cite{cohen2017emnist} nata 
per rendere il problema più difficile, viste le performance altissime
che le CNN ottengono con poche epoche di training sul MNIST originale.
Nasce dall'unione di altri dataset gestiti dal NIST, include cifre e
anche lettere sia maiuscole che minuscole, per un totale di 62 classi.
L'MNIST originale include cifre scritte da circa 250 persone diverse,
mentre l'EMNIST contiene dati da 3597 persone diverse e un totale di 
814'255 sample diversi.

Per finire, il FEMNIST (Federated EMINST)
viene introdotto in Caldas et al.~\cite{caldas2019femnist}
ed è un dataset che include tutti i sample dell'EMNIST partizionati
per scrittore originale, in modo da poter essere utilizzato in contesti
di apprendimento federato, in cui ogni client esegue il training non su
tutti i sample dell'EMNIST, ma solo quelli dello scrittore originale
corrispondente.

Come tutti i suoi predeccessori, il FEMNIST è un dataset che contiene
immagini di 28x28 pixel in greyscale, quindi ad un unico canale. Le
cifre e lettere sono tutte centrate nell'immagine in base centro di 
massa dei pixel del carattere.
Il dataset originale è stato costruito da, tra gli 
altri, dipendenti di google e come tale è stato pensato per funzionare
nativamente con Tensorflow. La repository originale per generare il
dataset può essere trovata su github ~\cite{leaf_repo}. Dato che però
il codice per questi esperimenti è stato scritto nel framework di PyTorch,
seguendo il dataset loader indicato su Papers with Code ~\cite{femnist_pwc},
per generare il dataset è stata usata la repository di Xiao Chenguang 
~\cite{femnist_hdf5}, che permette di generare un file \texttt{.hdf5} contenente
l'intero dataset, potendo anche scegliere se includere tutti i caratteri
o solo le cifre come nel MNIST originale. Quella che segue è la funzione
utilizzata per aprire il dataset \texttt{.hdf5} creato utilizzando questa repository
e istanziare i Dataset di PyTorch per ognuno dei client usati nella
simulazione:

\clearpage
\begin{lstlisting}
    def _get_femnist_datasets(
        num_writers: int,
        val_ratio: float,
        test_ratio: float,
        only_digits: bool = False,
    ) -> tuple[list[Dataset], list[Dataset], list[Dataset]]:    
        dataset_file = "write_digits.hdf5" if only_digits else "write_all.hdf5"
        full_dataset = h5py.File(f"data/{dataset_file}", "r")
        writers = sorted(full_dataset.keys())[:num_writers]
        train_sets = []
        val_sets = []
        test_sets = []
    
        for writer in writers:
            images = full_dataset[writer]["images"][:]
            labels = full_dataset[writer]["labels"][:]
            client_dataset = FemnistWriterDataset(
                images, labels, transform=femnist_transform
            )
    
            train_subset, val_subset, test_subset = _split_dataset(
                client_dataset, val_ratio, test_ratio
            )
            train_sets.append(train_subset)
            val_sets.append(val_subset)
            test_sets.append(test_subset)
    
        full_dataset.close()
        return train_sets, val_sets, test_sets
\end{lstlisting}

Tutti gli argomenti di questa funzione sono controllabili tramite la
configurazione Hydra usata per eseguire lo script. Si noti come
l'implementazione supporti anche un test di validazione per una 
possibile implementazione di early-stopping. I dataset di validazione 
sono però stati ignorati nel corso degli esperimenti.


\section{UCI HAR}
L'UCI HAR (University of California Irvine -
Human Activity Recognition) è un dataset diverso dal FEMNIST. In primis
non è un dataset di computer vision, ma di human activity recognition.
Il dataset è stato introdotto in ~\cite{Anguita2013APD} e la fonte dei
dati sono accellerometri e giroscopi di smartphones.

Negli esperimenti condotti 30 volontari di età compresa tra i 19 e i 48 anni
hanno compiuto diverse azioni avendo uno smartphone legato in vita 
usato per registrare l'accelerazione lineare e la velocità 
angolare 3-assiale. Lo smartphone usato è un Samsung Galaxy SII, le 
misurazioni sono state fatte ad una frequenza di 50Hz e le azioni compiute
sono il camminare, salire o scendere le scale, sedersi, alzarsi e 
sdraiarsi, per un totale di 6 classi di azioni diverse.
I sample del dataset non sono però sequenze di misurazioni; dopo 
aver misurato velocità e accelerazione, grandezze vettoriali 3-dimensionali,
e aver rimosso rumore dai segnali con un filtro Butterworth low-pass 
~\cite{butterworth_filter}, è stata fatta un'aggregazione di tutti i 
valori in una finestra temporale di 2.56s. Tale aggregazione produce 
561 valori reali che vengono organizzati in un unico feature vector 
che funziona da input del modello. La label è l'azione che stava venendo
compiuta in quei 2.56 secondi.
Scaricati i file del dataset, disponibile sul sito della 
University of California Irvine ~\cite{uci_har_ds}, sui file 
\texttt{README.txt} e \texttt{features\_info.txt}
si posso trovare informazioni dettagliate sul 
processing fatto per estrarre le 561 feature, mentre su youtube 
~\cite{har_measuring_video} è disponibile un video che mostra il 
processo di registrazione delle azioni.

Il dataset include un totale di 10299 vettori di feature che sono stati
già divisi randomicamente 
in un test set del 30\% del totale e in un training set 
che contiene il 70\% delle feature.
Quella che segue è la funzione utilizzata per aprire i file del 
dataset come sono forniti dall'UCI i Dataset di PyTorch per 
ognuno dei client 30 e partizionare correttamente i sample:

\clearpage
\begin{lstlisting}
def _load_har_dataset(train: bool):
    variant = "train" if train else "test"
    full_features = [[] for _ in range(30)]
    full_labels = [[] for _ in range(30)]

    # get subject ids for measurements
    with open(f"data/subject_{variant}.txt", "r") as subject_file:
        subject_ids = [int(line.strip()) - 1 for line in subject_file]
    # get features and match them with the user
    with open(f"data/X_{variant}.txt", "r") as feature_file:
        for i, line in enumerate(feature_file):
            features = [float(num) for num in line.split()]
            full_features[subject_ids[i]].append(features)
    # get labels and match them with the user
    with open(f"data/y_{variant}.txt", "r") as label_file:
        for i, line in enumerate(label_file):
            label = int(line)
            one_hot = torch.zeros(6, dtype=torch.float32)
            one_hot[label - 1] = label
            full_labels[subject_ids[i]].append(one_hot)

    if train:
        return full_features, full_labels
    # flatten in one dataset for testing
    else:
        return [
            feature for user_features in full_features for feature in user_features
        ], [label for user_labels in full_labels for label in user_labels]


def _get_har_datasets() -> tuple[list[Dataset], Dataset]:
    train_features, train_labels = _load_har_dataset(True)
    test_features, test_labels = _load_har_dataset(False)

    client_datasets = [
        HarDataset(
            features=torch.tensor(train_features[cid], dtype=torch.float32),
            labels=train_labels[cid],
        )
        for cid in range(len(train_features))
        # some clients dont include measurements in the trainset
        if len(train_features[cid]) > 0
    ]
    test_dataset = HarDataset(
        torch.tensor(test_features, dtype=torch.float32), test_labels
    )
    return client_datasets, test_dataset
\end{lstlisting}

In questo caso la logica per istanziare il dataset federato è stata 
divisa in due funzioni in quanto, diversamente dal FEMNIST in cui il 
dataset originale è già partizionato per client, in questo caso c'è 
bisogno di (i) aprire 3 file diversi \texttt{subject\_\{variant\}.txt}, 
\texttt{X\_\{variant\}.txt} e \texttt{y\_\{variant\}.txt} e riassociare 
i dati tra loro e (ii) instanziare i \texttt{Dataset} di PyTorch.
La funzione \texttt{\_load\_har\_dataset} si occupa di fare la prima tasks,
mentre \texttt{\_get\_har\_datasets} si occupa di istanziare correttamente le classi 
\texttt{Dataset}. Si noti che i dataset locali sono partrizionati per 
utente, mentre il dataset di test è un unico dataset con sample di 
tutti gli utenti.