\chapter{Miglioramenti}
Come già accennato, questo studio non ha l'obbiettivo di ottenere le 
migliori prestazioni possibili e come tale offre un ampio margine di 
miglioramento, specie nel caso in cui queste stesse tecniche vengano 
applicate a problemi più complessi.Un primo possibile miglioramento 
è quello di cambiare alcuni parametri 
della rete neurale. 


\subsubsection{Funzioni di attivazione}
La funzione di attivazione ReLU è 
estremamente popolare ed efficace nel mitigare il progblema del 
vanishing gradient, ma anch'essa ha le sue limitazioni come il 
problema del \textit{dying ReLU}. Alternative che possono essere 
esplorate sono la Leaky ReLU (LReLU) ~\cite{Maas2013RectifierNI},
definita come
\[
LReLU(z) = 
\begin{cases} 
      z, & \text{se } z > 0 \\
      \alpha z, & \text{altrimenti}
\end{cases}
\]
che ammette valori diversi da 0 per argomenti negativi e introduce un 
altro iperparametro \(\alpha\),
o la Parametric ReLU (PReLU) che rende l'iperparametro \(\alpha\)
un parametro \(a_i\) da imparare per ogni neurone:
\[
PReLU(z_i) = 
\begin{cases} 
      z_i, & \text{se } z_i > 0 \\
      a_i z_i, & \text{altrimenti}
\end{cases}
\]
Altre varianti della ReLU interessanti, in quanto funzioni liscie e 
differenziabili anche su 0, sono la Gaussian Error Linear Units (GELU)
~\cite{hendrycks2016gelu}
\[
GELU(z) = z P(Z \le z) = z \Phi(z)
\]
o la Exponential Linear Unit (ELU) ~\cite{clevert2016elu}, 
che assume anche valori negativi fino a -1
\[
ELU(z) = 
\begin{cases} 
      z, & \text{se } z > 0 \\
      \alpha (e^z - 1), & \text{altrimenti}
\end{cases}
\]
per un qualche iperparametro \alpha < 0.


\subsubsection{Regolarizzazione}
In questi esperimenti non si è fatto particolare uso di tecniche di 
normalizzazione o regolarizzazione, se non per una normalizzazione 
applicata alla immagini del FEMNIST ottenuta dalle trasformazioni 
built-in fornite da PyTorch. Si potrebbe allora provare alcune di 
queste tecniche come l'utilizzo della normalizzazione \(L_2\) in caso si 
usi SGD (\(L_2\) e weight decay sono equivalenti solo sotto SGD 
~\cite{Loshchilov2017AdamW}) oppure una delle varie tecniche di 
normalizzazione che velocizzano il training riducendo l'effetto del 
\textit{covariate shift}, come la batch normalization ~\cite{ioffe2015batch},
la layer normalization ~\cite{ba2016layer} o la group normalization 
~\cite{wu2018group}


\subsubsection{Scaling Up}
Un altro metodo efficace per migliorare le performace di una rete neurale 
è quello di scalare le dimensioni del modello, dei dataset o del training.
Operare con un'ampia mole di utenti e quindi con molte fonti di dati,
far produrre più dati per il training ove possibile o rendere i modelli 
più complessi possono essere opzioni percorribili. Bisogna tenere in 
considerazione il fatto che modelli più grandi possono incappare nel 
problema dell'overfitting, richiedere più risorse computazionali, 
rischiando di tagliare fuori alcuni dispositivi meno performati, e 
aumenta l'utilizzo della banda di rete per la comunicazione di un 
maggior numero di parametri.


\subsubsection{Inductive Bias migliori}
Un'altra recente area di studio nel campo del deep learning è quella 
della ricerca di architetture che per come sono costruite implementano
strutturalmente un'invarianza desiderata a qualche trasformazione 
dell'input. Un esempio concreto è l'invarianza traslazionale che 
implementano i layer convoluzionali attraverso la condivisione dei pesi.
Il geometric deep learning ~\cite{bronstein2021geometric} è lo studio di simmetrie 
(i.e. invarianti o equivarianti) geometriche che possono essere sfruttate nella 
progettazione di architetture neurali.
Il FEMNIST è un dataset particolare in cui tutti i caratteri sono stati
centrati e riscalati per ricoprire tutti la stessa area, cosa che in 
generale nella scrittura naturale è lontana dalla realtà. Può essere 
interessante esplorare architetture convoluzionali che siano invarianti 
anche a rotazioni o rescaling ~\cite{sosnovik2023symmetry, cohen2016group,
marcos2016rotation, cohen2016steer}.

Rimane però il fatto che le simmetrie desiderate dipendono fortemente 
dal problema trattato e per tale ragione vanno analizzate per lo 
specifico problema preso in considerazione.
